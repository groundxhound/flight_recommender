{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example Usecase for Movies\n",
    "https://www.kernix.com/blog/recommender-system-based-on-natural-language-processing_p10\n",
    "\n",
    "Used algorithm LSI (LSA). \n",
    "Idea: Texts that contain similar words have a similar meaning.\n",
    "\n",
    "## Preprocessing\n",
    "  \n",
    "We create a so called bag of words. This means that for each text we throw all words into a \"bag\" so we ignore the ordering and just look at which words occur how often. This can be thought of as a matrix where each row corresponds to a word and each column is a text. The value written is either 0, 1 for occured or did not occur or the number of occurences or the tf-idf value (text frequency - inverse document frequency).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iot?', 'is', 'that', 'about', 'text', 'blockchain.', 'this', 'a']\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "texts = [\"This is a text about blockchain.\", \"Is that a text about IoT?\"]\n",
    "\n",
    "def split_words(texts):\n",
    "  words = set()\n",
    "  for t in texts:\n",
    "    words = words.union(t.lower().split(\" \"))\n",
    "    \n",
    "  words = list(words)\n",
    "  return words\n",
    "  \n",
    "words = split_words(texts)\n",
    "print(words)\n",
    "print(np.array([[int(w in s) for s in texts] for w in words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking tolower we identyfied that \"Is\" and \"is\" are the same words but for example \"text.\" and \"text\" are seen as different. So we want to do an additional step where we delete non words. This can be done easily by some regex.\n",
    "Still words like \"book\" and \"books\" or \"walk\" and \"walked\" are seen as different. To eliminate those differences we need some smarter language specific algorithms. This is called stemming, example library: snowball.\n",
    "For some details see http://snowball.tartarus.org/texts/introduction.html\n",
    "(Im pretty sure there is some neural network solution for this too. ~1980-1990 technology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'book', 'walk', 'walk', 'die', 'die', 'happi', 'unhappi', 'becom', 'becam']\n",
      "['money', 'cash', 'cheapli', 'repli', 'sun', 'sunshin', 'dictat', 'dictatorship', 'hous', 'huos']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "words = [\"book\", \"books\", \"walk\", \"walked\", \"die\", \"dying\", \"happy\", \"unhappy\",\n",
    "\"become\", \"became\"]\n",
    "words2 = [\"money\", \"cash\", \"cheaply\", \"reply\", \"sun\", \"sunshine\",\n",
    "\"dictator\", \"dictatorship\", \"house\", \"huose\", ]\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "stemmed = [stemmer.stem(w) for w in words]\n",
    "stemmed2 = [stemmer.stem(w) for w in words2]\n",
    "print(stemmed)\n",
    "print(stemmed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say better than what i could have implemented and definetly useful but has some serious limitations.  \n",
    "  \n",
    "  \n",
    "Also if you look at the similarities of the first example then it would show that these two texts are quite similar because they have the meaningless words \"is,a, about\" in common. Another preprocessing step is to delete such useless words (stopwords).  \n",
    "We do this by just taking a list of known english stopwords and delete those from our texts. (static)\n",
    "  \n",
    "Usually this word text matrix is really spars so instead of keeping a trillion 0 in memory we use a sparse matrix notation. Saving only the (row_number, column_number, value) where the value is not 0. Storing 3 $\\cdot$ nr_non_zeroes instead of rows $\\cdot$ columns. This is also called corpus.\n",
    "  \n",
    "By having this translationg we get word vecs for each text and we could just measure how similar two of these vecs are. This would be a algorithm that hasnt learned anything from the data though.\n",
    "  \n",
    "## Creating an LSI model\n",
    "The idea is to reduce dimensions and learn topics. So that the algorithm can learn words that are similar and not only check if two texts have the same words in it. So if there is a text like \"bmw is a car\" and one with \"vw is a car\" that it will learn the topic \"bmw car vw\" and if we get two texts \"i have a bmw\" and \"i want a vw\" that thw algorithm can now knwo that they both talk about cars while just comparing the bag of words vectors of those two would not show similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bmw', 'are', 'cars'], ['vw', 'is', 'a', 'car'], ['sun', 'and', 'beach']]\n",
      "[['bmw', 'car'], ['vw', 'car'], ['sun', 'beach']]\n",
      "{'beach', 'bmw', 'sun', 'vw', 'car'}\n",
      "[[0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_to_mat' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4ae0cdcb93ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mwords_text_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_words\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdistinct_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_text_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mwords_text_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_text_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_uv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_to_mat' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "row_words = [\"bmw\", \"sun\", \"car\", \"beach\", \"vw\"]\n",
    "texts = [\"bmw are cars\", \"vw is a car\", \"sun and beach\"]\n",
    "\n",
    "texts = [t.split(\" \") for t in texts]\n",
    "print(texts)\n",
    "\n",
    "def stem_and_stop(texts):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [[stemmer.stem(w.lower()) for w in words] for words in texts]\n",
    "    stopwords_set = set(stopwords.words(\"English\"))\n",
    "    final_words = [[w for w in words if w not in set(stopwords.words(\"English\"))] for words in stemmed]\n",
    "    return final_words\n",
    "\n",
    "final_words = stem_and_stop(texts)\n",
    "distinct_words = set([w for text in final_words for w in text])\n",
    "print(final_words)\n",
    "print(distinct_words)\n",
    "words_text_mat = np.array([[int(w in s) for s in final_words] for w in distinct_words])\n",
    "print(words_text_mat)\n",
    "words_text_mat = text_to_mat(texts)\n",
    "\n",
    "u, s, vh = np.linalg.svd(words_text_mat, compute_uv=True, full_matrices=False)\n",
    "u = np.round(u, 1)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of this matrix are the topics ordered by importance. As we can see the most important topic is actually the topic containing vw,bmw and car and the second most important topic contains the other words sun and beach.  \n",
    "  \n",
    "So the algorithm learned that vw and bmw are both cars or atleast made a connection between those. If we score the similarity between the two new texts \"bmw in the sun\" and \"vw on the beach\". Then these are translatet into word vectors as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bmw', 'in', 'the', 'sun'], ['vw', 'on', 'the', 'beach']]\n",
      "[['bmw', 'sun'], ['vw', 'beach']]\n",
      "{'vw', 'beach', 'bmw', 'sun'}\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "new_texts = [\"bmw in the sun\", \"vw on the beach\"]\n",
    "\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682 number of words used from texts.\n"
     ]
    }
   ],
   "source": [
    "from recommender.nlp import LanguageProcessing\n",
    "from recommender.database import Database\n",
    "d = Database()\n",
    "L = LanguageProcessing(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 topics\n[('actual', -0.02), ('african', -0.02), ('altern', -0.02), ('ambiti', -0.02)]\n[('mediev', 0.24), ('second', 0.2), ('largest', 0.18), ('million', 0.16)]\n[('destin', 0.2), ('econom', 0.18), ('excit', 0.12), ('divid', 0.11)]\n[('island', 0.22), ('boat', 0.19), ('resort', 0.18), ('water', 0.15)]\n[('templ', 0.13), ('divers', 0.12), ('sophist', 0.12), ('mix', 0.11)]\n[('largest', 0.22), ('second', 0.16), ('million', 0.12), ('develop', 0.11)]\n[('industri', 0.11), ('major', 0.1), ('past', 0.1), ('light', 0.1)]\n[('germani', 0.18), ('mediev', 0.17), ('especi', 0.12), ('artist', 0.11)]\n[('two', 0.14), ('meet', 0.13), ('th', 0.13), ('contain', 0.11)]\n[('germani', 0.16), ('mediev', 0.12), ('entir', 0.11), ('year', 0.1)]\n"
     ]
    }
   ],
   "source": [
    "#L.ldamodel.get_document_topics()\n",
    "import numpy as np\n",
    "u = L.lsi.get_topics()\n",
    "ur = np.round(u, 2)\n",
    "nr_words = 4\n",
    "print(\"Top 10 topics\")\n",
    "def get_topics(v1, sgn):\n",
    "    x = [(L.dictionary[i], v1[i]) for i in range(len(v1))]# if abs(v1[i]) > 0.1]\n",
    "    x.sort(key=lambda a:a[1], reverse=True)\n",
    "    if sgn:\n",
    "            #sum([abs(a[1]) for a in x[0:3]]) >= sum([abs(a[1]) for a in x[-nr_words:]]):\n",
    "        print(x[0:nr_words])\n",
    "    else:\n",
    "        tmp = x[-nr_words:]\n",
    "        tmp.reverse()\n",
    "        print(tmp)\n",
    "for i in range(10):\n",
    "    v1 = ur[i, :]\n",
    "\n",
    "    get_topics(v1, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 682)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L.destinations.index[L.destinations[\"iata_code\"] == \"BGI\"][0]\n",
    "L.model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nNYC\n[('island', 0.11282643595522328), ('neighborhood', 0.10605907111502684), ('night', 0.0920180156931245), ('find', 0.09075306829243007)]\n[('skyscrap', 0.12507103093818855), ('templ', 0.12260450233617123), ('wealth', 0.11417418140488549), ('industri', 0.09942105487281615)]\n[('bar', -0.09495541593415996), ('find', -0.09172510242538237), ('restaur', -0.08861597197908572), ('area', -0.0870666638122343)]\n\nBER\n[('student', -0.15092256943635038), ('canal', -0.14263010315098262), ('plenti', -0.10913071893928598), ('event', -0.10782753695062837)]\n[('industri', 0.11209655780109015), ('light', 0.09948622828407254), ('germani', 0.09667178883892408), ('major', 0.0960377820784455)]\n[('bar', -0.09495541593415996), ('find', -0.09172510242538237), ('restaur', -0.08861597197908572), ('area', -0.0870666638122343)]\n\nLON\n[('innov', -0.13951511411982068), ('landscap', -0.13319172611706984), ('magnific', -0.13223276769097111), ('build', -0.12780575152787896)]\n[('america', -0.15148983957366766), ('olymp', -0.11733708728445508), ('classic', -0.11264209487171074), ('perform', -0.10817020215262706)]\n[('bar', -0.09495541593415996), ('find', -0.09172510242538237), ('restaur', -0.08861597197908572), ('area', -0.0870666638122343)]\n\nBGI\n[('least', 0.14928394099404044), ('fish', 0.12876885317516218), ('student', 0.12499300756776444), ('second', 0.11573216946398825)]\n[('island', 0.11282643595522328), ('neighborhood', 0.10605907111502684), ('night', 0.0920180156931245), ('find', 0.09075306829243007)]\n[('bar', -0.09495541593415996), ('find', -0.09172510242538237), ('restaur', -0.08861597197908572), ('area', -0.0870666638122343)]\n\nPMI\n[('mediev', -0.2327273924811002), ('church', -0.12214925076525977), ('island', -0.11583987587768575), ('baroqu', -0.11340023165408729)]\n[('canal', 0.12263586719240747), ('bar', 0.12077433305371728), ('squar', 0.11931247630309035), ('earli', 0.11055056571907708)]\n[('bar', -0.09495541593415996), ('find', -0.09172510242538237), ('restaur', -0.08861597197908572), ('area', -0.0870666638122343)]\n\nHKG\n[('island', 0.1457298276787623), ('hall', 0.11660242702791891), ('look', 0.10990974559990198), ('industri', 0.1090174350952243)]\n[('scene', 0.13766297906432992), ('eateri', 0.13008530437388818), ('destin', 0.12614416081742114), ('germani', 0.1129590278728156)]\n[('bar', -0.09495541593415996), ('find', -0.09172510242538237), ('restaur', -0.08861597197908572), ('area', -0.0870666638122343)]\n"
     ]
    }
   ],
   "source": [
    "for city_name in [\"NYC\", \"BER\", \"LON\", \"BGI\", \"PMI\", \"HKG\"]:\n",
    "    print(\"\\n\" + city_name)\n",
    "    city_index = L.destinations.index[L.destinations[\"iata_code\"] == city_name][0]\n",
    "    topic_vec=[x[1] for x in L.lsi[L.corpus[city_index]]]\n",
    "    topic_vec_abs = np.abs(topic_vec)\n",
    "    topic_indices = np.argpartition(topic_vec_abs, [-1,-2,-3])[-3:]\n",
    "    u = L.lsi.get_topics()\n",
    "    for i in topic_indices:\n",
    "        sgn = topic_vec[i] > 0\n",
    "        ur = u[i,:]\n",
    "        get_topics(ur,sgn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 5, 'tfidf': True}\n3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 5, 'tfidf': False}\n1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LDA', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 5, 'tfidf': True}\n0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LDA', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 5, 'tfidf': False}\n1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 15, 'tfidf': True}\n1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 15, 'tfidf': False}\n1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LDA', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 15, 'tfidf': True}\n0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LDA', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 15, 'tfidf': False}\n0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 30, 'tfidf': True}\n0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 30, 'tfidf': False}\n0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LDA', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 30, 'tfidf': True}\n1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LDA', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 30, 'tfidf': False}\n0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 60, 'tfidf': True}\n0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 60, 'tfidf': False}\n1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LDA', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 60, 'tfidf': True}\n1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 number of words used from texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'LDA', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 60, 'tfidf': False}\n0\nbest config is: {'algorithm': 'LSI', 'min_word_count': 6, 'no_above_fraction': 0.4, 'delete_numbers': True, 'delete_words': ['also'], 'nr_topics': 5, 'tfidf': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'LSI',\n 'delete_numbers': True,\n 'delete_words': ['also'],\n 'min_word_count': 6,\n 'no_above_fraction': 0.4,\n 'nr_topics': 5,\n 'tfidf': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recommender.nlp import LanguageProcessing\n",
    "from recommender.database import Database\n",
    "d = Database()\n",
    "L = LanguageProcessing(d)\n",
    "L.optimize_parameters()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
